{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db28d83f-1290-4712-9a80-5e272f928152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python: D:\\LONGSPUR\\TASK\\AI_buddy_model\\buddyvenv\\Scripts\\python.exe\n",
      "Virtual Env: D:\\LONGSPUR\\TASK\\AI_buddy_model\\buddyvenv\n",
      "Working Dir: D:\\LONGSPUR\\TASK\\AI_buddy_model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(f\"\"\"\n",
    "Python: {sys.executable}\n",
    "Virtual Env: {sys.prefix}\n",
    "Working Dir: {os.getcwd()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682e529a-275e-4a1f-b09d-1075af280fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\LONGSPUR\\TASK\\AI_buddy_model\\buddyvenv\\Lib\\site-packages\\pandas\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__file__)  # Should show buddyvenv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1f9ff1-c708-4b7b-ac76-a519fdba4b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2163967-2d95-4273-aa0e-261966d0bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --force-reinstall python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71046d2c-ef8a-402b-990c-f5dcfe4aafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a0f2a9-7f45-4aa3-9193-a9c2a0c4093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')  \n",
    "if openai_api_key:\n",
    "    print(\"API key loaded successfully!\")\n",
    "else:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf98451f-4a29-4b55-b697-755fd890ba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.70.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: Apache-2.0\n",
      "Location: D:\\LONGSPUR\\TASK\\AI_buddy_model\\buddyvenv\\Lib\\site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc71bb9-4766-4c64-9c2f-702197087420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "#from langchain_core.documents import Document\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f683-cec8-4e79-978f-ccddf3e9c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca32697-d82a-42a4-a9f2-e80716808460",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyPDF2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f8d74-85b2-41f6-be51-1dd20c773a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess_pdf import extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3824c-e46e-4a69-a2b9-d588af160902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = os.path.join(\"knowledge-base\", \"textbook.pdf\")\n",
    "pdf_path = os.path.abspath(base_path) \n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "if text:\n",
    "    print(text[:100])\n",
    "else:\n",
    "    print(\"No text extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c42a7-f550-4477-9875-47320c63f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ADVANCED = False  \n",
    "MODEL = \"gpt-4o-mini\" if USE_ADVANCED else \"gpt-3.5-turbo\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bb03f-f400-437f-8c8a-3179185ea9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c4f09-0dfb-4110-ac49-c879bc83337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk_data_dir = \"D:\\\\LONGSPUR\\\\TASK\\\\AI_buddy_model\\\\nltk_data\"\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a33c8-8de6-48fd-889a-7414fb218eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import encoding_for_model\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013c404-24bd-4200-9598-e4f88ac29b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.chunking import dynamic_chunking_with_metadata\n",
    "chunks = dynamic_chunking_with_metadata(text, token_limit=1000, model=\"cl100k_base\",page_number=1)\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(\"First Chunk:\")\n",
    "print(chunks[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfc7f5-1a85-4e7e-b76d-15376503a05c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chunk_lengths = [len(chunk['text']) for chunk in chunks]\n",
    "token_counts = [chunk['tokens'] for chunk in chunks]\n",
    "\n",
    "print(\"🔍 Sample Chunk Analysis\")\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"Chunk {i} | Tokens: {chunk['tokens']}\\n{chunk['text'][:200]}...\\n{'-'*50}\")\n",
    "\n",
    "# Optional: plot\n",
    "plt.hist(token_counts, bins=20)\n",
    "plt.title(\"Token Count Distribution in Chunks\")\n",
    "plt.xlabel(\"Tokens per Chunk\")\n",
    "plt.ylabel(\"Number of Chunks\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75cf76-73f9-431d-828a-b29381c9ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize_chunks(chunks):\n",
    "    df = pd.DataFrame([{\n",
    "        \"Chunk ID\": c[\"chunk_id\"],\n",
    "        \"Tokens\": c[\"tokens\"],\n",
    "        \"Preview\": c[\"text\"][:100] + \"...\" if len(c[\"text\"]) > 100 else c[\"text\"]\n",
    "    } for c in chunks])\n",
    "\n",
    "    display(df)\n",
    "    print(f\"📦 Total Chunks: {len(chunks)}\")\n",
    "    print(f\"🔠 Total Tokens: {sum(c['tokens'] for c in chunks)}\")\n",
    "\n",
    "visualize_chunks(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01471483-854a-4f59-a82b-21fbd2063fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23279b09-fbe4-4fe9-8d07-e00aa9890efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d835a-2b27-480c-bd91-b04e9d3139b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9fe7a-d5eb-4969-af1e-0f837921b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-chroma -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e6870-e994-42ec-8007-fea82f77dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaad616-39a2-4876-b033-b4e1fec73ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "if os.path.exists(db_name):\n",
    "    confirm = input(f\"⚠️ ChromaDB exists at '{db_name}'. Do you want to delete it? (y/n): \")\n",
    "    if confirm.lower() == \"y\":\n",
    "        Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "        print(f\"🗑️ ChromaDB at '{db_name}' has been deleted.\")\n",
    "    else:\n",
    "        print(\"❌ Skipped deletion. Existing DB will be reused.\")\n",
    "else:\n",
    "    print(f\"✅ No existing ChromaDB found at '{db_name}'. Starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b9089c-76c1-4aa8-8acf-92e70f1a435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=chunk[\"text\"],\n",
    "        metadata={\n",
    "            \"chunk_id\": chunk[\"chunk_id\"],\n",
    "            \"page_number\": chunk[\"page_number\"],\n",
    "            \"tokens\": chunk[\"tokens\"]\n",
    "        }\n",
    "    )\n",
    "    for chunk in chunks\n",
    "]\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./vector_db\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vector DB rebuilt and persisted.\")\n",
    "print(\"🔍 Vectorstore test — trying to retrieve chunks for: 'solar system'\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "results = retriever.get_relevant_documents(\"solar system\")\n",
    "\n",
    "print(f\"✅ Retrieved {len(results)} chunks\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Chunk {i}: {doc.page_content[:150]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13920a-b36f-4b63-b982-a3c650b250e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-openai chromadb openai tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56bac8-7334-4816-aa17-24220b02ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rank_bm25 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41a1a9-1eed-4c8a-929f-e5e272d41ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "vector_retriever = vectorstore.as_retriever()\n",
    "vector_retriever.search_kwargs['k'] = 5\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c724c82-b4d7-458c-827d-27943b9fc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe45d16-e527-4a03-af39-14180813195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f8079-3d0c-4b2c-93ef-09b0f2f1846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4o-mini\")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True  # ✅ This is the key!\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833e9ce-73a4-4a89-95bf-07a2a2e3296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"is pdf contains image representation of solar system\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3929e23-57d6-443e-9406-ba1318ac7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"textbook\": \"You are an educational assistant. Respond only in English. Generate the answer strictly using the most relevant retrieved chunks from the textbook PDF. Do not include any additional explanation or information not found in the retrieved content.\",\n",
    "    \n",
    "    \"detailed\": \"You are an educational assistant. Respond only in English. Using the retrieved textbook content as the main source, explain this in a clearer and slightly more detailed way...\",\n",
    "    \n",
    "    \"advanced\": \"You are an educational assistant. Respond only in English. Now go beyond the textbook. Provide a deeper, structured explanation...\"\n",
    "}\n",
    "\n",
    "def build_prompt(question, level):\n",
    "    # Clean the input\n",
    "    question = str(question).strip()\n",
    "    prompt_prefix = PROMPTS.get(level, PROMPTS[\"textbook\"])  # Default to textbook if invalid level\n",
    "    \n",
    "    # Build the complete prompt\n",
    "    full_prompt = f\"{prompt_prefix} {question}\"\n",
    "    \n",
    "    return full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3deb0-a054-434c-ac31-930883104f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "SUGGEST_QUESTIONS_PROMPT = \"\"\"\n",
    "Based on the following response, suggest 3 interesting follow-up questions a student might ask to understand more:\n",
    "\n",
    "Response:\n",
    "\"{content}\"\n",
    "\n",
    "Respond with each question on a new line.\n",
    "\"\"\"\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba0ee8-18be-43e9-9fcf-ec8fd627729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_stream(message, history, level, last_question):\n",
    "    source_chunks = []\n",
    "\n",
    "    if not message or len(message.strip()) < 4:\n",
    "        if not last_question or len(last_question.strip()) < 4:\n",
    "            yield history, history, last_question, source_chunks\n",
    "            return\n",
    "        message = last_question\n",
    "    else:\n",
    "        last_question = message\n",
    "\n",
    "    print(f\"📩 User question: {message}\")\n",
    "    print(f\"🎯 Level: {level}\")\n",
    "\n",
    "    try:\n",
    "        if level == \"textbook\":\n",
    "            print(\"📘 Using textbook retriever\")\n",
    "            result = conversation_chain.invoke({\n",
    "                \"question\": message,\n",
    "                \"chat_history\": history[-10:]\n",
    "            })\n",
    "            answer = result[\"answer\"]\n",
    "            source_chunks = result.get(\"source_documents\", [])\n",
    "            \n",
    "            print(\"✅ Retrieved source_chunks:\", len(source_chunks))\n",
    "            for i, doc in enumerate(source_chunks):\n",
    "                print(f\"Chunk {i}: {doc.page_content[:100]}\")\n",
    "\n",
    "\n",
    "        elif level == \"detailed\":\n",
    "            print(\"📗 Using LLM to explain textbook content\")\n",
    "            base_result = conversation_chain.invoke({\n",
    "                \"question\": message,\n",
    "                \"chat_history\": history[-10:]\n",
    "            })\n",
    "            base_answer = base_result[\"answer\"]\n",
    "\n",
    "            llm_detail = ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                streaming=True,\n",
    "                temperature=0.5,\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                callbacks=[StreamingStdOutCallbackHandler()]\n",
    "            )\n",
    "\n",
    "            detailed_prompt = build_prompt(base_answer, \"detailed\")\n",
    "            final_answer = \"\"\n",
    "            for chunk in llm_detail.stream([HumanMessage(content=detailed_prompt)]):\n",
    "                final_answer += chunk.content\n",
    "                answer = final_answer\n",
    "                yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], last_question, source_chunks\n",
    "            return\n",
    "\n",
    "        elif level == \"advanced\":\n",
    "            print(\"📕 Using full LLM + placeholder for media enrichment\")\n",
    "            llm_adv = ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                streaming=True,\n",
    "                temperature=0.7,\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                callbacks=[StreamingStdOutCallbackHandler()]\n",
    "            )\n",
    "\n",
    "            advanced_prompt = build_prompt(message, \"advanced\")\n",
    "            final_answer = \"\"\n",
    "            for chunk in llm_adv.stream([HumanMessage(content=advanced_prompt)]):\n",
    "                final_answer += chunk.content\n",
    "                answer = final_answer\n",
    "                yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], last_question, source_chunks\n",
    "            return\n",
    "\n",
    "        # Shared for all levels\n",
    "        new_messages = [\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        history += new_messages\n",
    "        yield history, history, last_question, source_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: {e}\")\n",
    "        error_msg = f\"⚠️ Something went wrong: {str(e)}\"\n",
    "        yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": error_msg}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": error_msg}], last_question, source_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a609b3-30ba-47dd-9b83-100358324eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & Load Chat Functions\n",
    "def save_chat_to_file(history):\n",
    "    output_dir = \"D:/BuddyChats\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"Buddy_Chat_{timestamp}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"📁 Saved file to:\", filepath)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_chat_from_file(file_obj):\n",
    "    if not file_obj or not hasattr(file_obj, \"name\"):\n",
    "        print(\"⚠️ No file uploaded\")\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "        with open(file_obj.name, \"r\", encoding=\"utf-8\") as f:\n",
    "            history = json.load(f)\n",
    "\n",
    "        if isinstance(history, list):\n",
    "            if all(isinstance(pair, list) and len(pair) == 2 for pair in history):\n",
    "                message_format = []\n",
    "                for pair in history:\n",
    "                    message_format.append({\"role\": \"user\", \"content\": pair[0]})\n",
    "                    message_format.append({\"role\": \"assistant\", \"content\": pair[1]})\n",
    "                return message_format, message_format\n",
    "            elif all(isinstance(m, dict) and \"role\" in m and \"content\" in m for m in history):\n",
    "                return history, history\n",
    "\n",
    "        print(\"⚠️ Unsupported format in loaded chat\")\n",
    "        return [], []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading chat file: {e}\")\n",
    "        return [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1dbef2-4db9-49dd-a794-c84beb374cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suggested_questions(content, level=\"textbook\", source_chunks=None):\n",
    "    try:\n",
    "        print(f\"🔍 Level: {level}\")\n",
    "        print(f\"📥 Received content: {content[:100]}...\")\n",
    "\n",
    "        # If textbook, use only source_chunks\n",
    "        if level == \"textbook\":\n",
    "            if not source_chunks:\n",
    "                print(\"⚠️ No source_chunks passed.\")\n",
    "                return [\"Sorry, I couldn't find anything relevant in the textbook.\", \"\", \"\"]\n",
    "\n",
    "            context_text = \"\\n\\n\".join([\n",
    "                chunk.page_content.strip()\n",
    "                for chunk in source_chunks\n",
    "                if hasattr(chunk, \"page_content\") and chunk.page_content and len(chunk.page_content.strip()) > 30\n",
    "            ])\n",
    "\n",
    "            if not context_text:\n",
    "                print(\"⚠️ No usable content in source_chunks.\")\n",
    "                return [\"Sorry, I couldn't find anything relevant in the textbook.\", \"\", \"\"]\n",
    "\n",
    "            print(\"✅ Building prompt from chunked content.\")\n",
    "            prompt = f\"\"\"\n",
    "You are a helpful educational assistant. Only use the following textbook content to answer.\n",
    "Don't use any external knowledge. Your task is to generate 3 simple, factual follow-up questions that help a student explore further.\n",
    "\n",
    "Textbook Content:\n",
    "\\\"\\\"\\\"{context_text}\\\"\\\"\\\"\n",
    "\n",
    "Give 3 questions. One per line.\n",
    "\"\"\"\n",
    "        else:\n",
    "            # For other levels, use the assistant’s answer\n",
    "            prompt = f\"\"\"\n",
    "You're a helpful tutor. Based on the following explanation, suggest 3 follow-up questions a student might ask to explore further.\n",
    "\n",
    "Answer:\n",
    "\\\"\\\"\\\"{content}\\\"\\\"\\\"\n",
    "\n",
    "Give 3 questions. One per line.\n",
    "\"\"\"\n",
    "\n",
    "        # Run LLM\n",
    "        llm = ChatOpenAI(\n",
    "            temperature=0.7,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        response = llm.invoke(prompt)\n",
    "        suggestions = response.content.strip().split(\"\\n\")\n",
    "        cleaned = [s.strip(\" -•123.\").strip() for s in suggestions if s.strip()]\n",
    "        while len(cleaned) < 3:\n",
    "            cleaned.append(\"\")\n",
    "        return cleaned[:3]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating suggestions: {str(e)}\")\n",
    "        return [\"\", \"\", \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c41f0b-0f30-48b6-9464-10c3ce8ee3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# UI\n",
    "with gr.Blocks(css=... ) as demo:  # Keep your existing CSS here\n",
    "\n",
    "    gr.Markdown(\"### 👋 Welcome to Buddy AI: Your Learning Companion\")\n",
    "\n",
    "    question = gr.Textbox(label=\"Ask your question 👇\", lines=1)\n",
    "    level = gr.Dropdown(choices=[\"textbook\", \"detailed\", \"advanced\"], value=\"textbook\", label=\"Answer Level 🎯\")\n",
    "    chat_ui = gr.Chatbot(label=\"🧠 Buddy AI Conversation\", type=\"messages\")\n",
    "    state = gr.State([])\n",
    "    last_question = gr.State(\"\") \n",
    "    source_chunk_state = gr.State()  # ✅ Store retrieved chunks for suggested questions\n",
    "    suggested_q1 = gr.State()\n",
    "    suggested_q2 = gr.State()\n",
    "    suggested_q3 = gr.State()\n",
    "    \n",
    "\n",
    "\n",
    "    suggest_heading = gr.Markdown(\"#### 🤔 Suggested Follow-up Questions\", visible=False)\n",
    "    suggest_btn1 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "    suggest_btn2 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "    suggest_btn3 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            submit_btn = gr.Button(\"✅ Submit\", elem_classes=\"gr-button-primary\")\n",
    "            clear_btn = gr.Button(\"❌ Clear\")\n",
    "        with gr.Column(scale=1):\n",
    "            save_btn = gr.Button(\"💾 Save Chat\")\n",
    "            save_status = gr.Markdown(visible=False)\n",
    "            load_file = gr.File(label=\"📂 Load Previous Chat (.json)\", file_types=[\".json\"])\n",
    "            load_btn = gr.Button(\"📂 Load Chat\")\n",
    "\n",
    "    chat_event = submit_btn.click(\n",
    "        fn=chat_stream,\n",
    "        inputs=[question, state, level, last_question],\n",
    "        outputs=[chat_ui, state, question, source_chunk_state],\n",
    "        api_name=\"chat_stream\"\n",
    "    ).then(\n",
    "        fn=lambda q: q,  # update last_question\n",
    "        inputs=[question],\n",
    "        outputs=[last_question]\n",
    ")\n",
    "     # Auto-submit when answer level changes\n",
    "    \n",
    "    def extract_last_bot_reply(chat_history):\n",
    "        if isinstance(chat_history, list):\n",
    "            if len(chat_history) > 0 and isinstance(chat_history[-1], list):\n",
    "                return chat_history[-1][1]\n",
    "            for msg in reversed(chat_history):\n",
    "                if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "                    return msg.get(\"content\", \"\")\n",
    "        return \"\"\n",
    "    def suggest_followups(chat_history, level=\"textbook\", source_chunks=None):\n",
    "        print(\"📨 Suggestion Triggered\")\n",
    "        print(f\"Level: {level}\")\n",
    "        print(f\"Chunks received: {len(source_chunks) if source_chunks else 0}\")\n",
    "        \n",
    "        last_bot_reply = extract_last_bot_reply(chat_history)\n",
    "        suggestions = generate_suggested_questions(last_bot_reply, level, source_chunks)\n",
    "        \n",
    "        s1 = gr.update(value=suggestions[0], visible=True) if suggestions else gr.update(visible=False)\n",
    "        s2 = gr.update(value=suggestions[1], visible=True) if len(suggestions) > 1 else gr.update(visible=False)\n",
    "        s3 = gr.update(value=suggestions[2], visible=True) if len(suggestions) > 2 else gr.update(visible=False)\n",
    "        return s1, s2, s3, suggestions[0], suggestions[1], suggestions[2], gr.update(visible=True)\n",
    "\n",
    "    \n",
    "    chat_event.then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui, level, source_chunk_state],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "    level.change(\n",
    "        fn=chat_stream,\n",
    "        inputs=[question, state, level, last_question],\n",
    "        outputs=[chat_ui, state, question, source_chunk_state]\n",
    "    ).then(\n",
    "        fn=lambda q: q,\n",
    "        inputs=[question],\n",
    "        outputs=[last_question]\n",
    "    ).then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui, level, source_chunk_state],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "\n",
    "    suggest_btn1.click(chat_stream, inputs=[suggested_q1, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "    suggest_btn2.click(chat_stream, inputs=[suggested_q2, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "    suggest_btn3.click(chat_stream, inputs=[suggested_q3, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda: ([], [], \"\", \"\", \"\", \"\", \"\", \"\", \"\", gr.update(visible=False)),\n",
    "        inputs=[],\n",
    "        outputs=[chat_ui, state, question, suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "    save_btn.click(\n",
    "        fn=save_chat_to_file,\n",
    "        inputs=[state],\n",
    "        outputs=[save_status]\n",
    "    ).then(\n",
    "        fn=lambda f: gr.update(visible=True, value=f\"✅ Chat saved to: `{f}`\"),\n",
    "        inputs=[save_status],\n",
    "        outputs=[save_status]\n",
    "    )\n",
    "\n",
    "    load_btn.click(\n",
    "        fn=load_chat_from_file,\n",
    "        inputs=[load_file],\n",
    "        outputs=[chat_ui, state]\n",
    "    ).then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "demo.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127b69a-48b3-48c1-a063-4b9eb804dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f52af3-3a33-4db6-a5a6-2b4fce41e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf2598-89cd-43fc-beec-0bd4a654c412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db28d83f-1290-4712-9a80-5e272f928152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python: D:\\LONGSPUR\\TASK\\AI_buddy_model\\buddyvenv\\Scripts\\python.exe\n",
      "Virtual Env: D:\\LONGSPUR\\TASK\\AI_buddy_model\\buddyvenv\n",
      "Working Dir: D:\\LONGSPUR\\TASK\\AI_buddy_model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(f\"\"\"\n",
    "Python: {sys.executable}\n",
    "Virtual Env: {sys.prefix}\n",
    "Working Dir: {os.getcwd()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682e529a-275e-4a1f-b09d-1075af280fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\LONGSPUR\\TASK\\AI_buddy_model\\buddyvenv\\Lib\\site-packages\\pandas\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__file__)  # Should show buddyvenv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1f9ff1-c708-4b7b-ac76-a519fdba4b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2163967-2d95-4273-aa0e-261966d0bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --force-reinstall python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71046d2c-ef8a-402b-990c-f5dcfe4aafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a0f2a9-7f45-4aa3-9193-a9c2a0c4093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')  \n",
    "if openai_api_key:\n",
    "    print(\"API key loaded successfully!\")\n",
    "else:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf98451f-4a29-4b55-b697-755fd890ba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.70.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: Apache-2.0\n",
      "Location: D:\\LONGSPUR\\TASK\\AI_buddy_model\\buddyvenv\\Lib\\site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc71bb9-4766-4c64-9c2f-702197087420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "#from langchain_core.documents import Document\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c797f683-cec8-4e79-978f-ccddf3e9c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca32697-d82a-42a4-a9f2-e80716808460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f18f8d74-85b2-41f6-be51-1dd20c773a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess_pdf import extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf3824c-e46e-4a69-a2b9-d588af160902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyond Earth12Chapter\n",
      "Nubra is a beautiful region in Ladakh. An eleven-year old \n",
      "girl Yangdol and he\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_path = os.path.join(\"knowledge-base\", \"textbook.pdf\")\n",
    "pdf_path = os.path.abspath(base_path) \n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "if text:\n",
    "    print(text[:100])\n",
    "else:\n",
    "    print(\"No text extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6c42a7-f550-4477-9875-47320c63f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ADVANCED = False  \n",
    "MODEL = \"gpt-4o-mini\" if USE_ADVANCED else \"gpt-3.5-turbo\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818bb03f-f400-437f-8c8a-3179185ea9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3c4f09-0dfb-4110-ac49-c879bc83337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:\\LONGSPUR\\TASK\\AI_buddy_model\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk_data_dir = \"D:\\\\LONGSPUR\\\\TASK\\\\AI_buddy_model\\\\nltk_data\"\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b69a33c8-8de6-48fd-889a-7414fb218eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import encoding_for_model\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0013c404-24bd-4200-9598-e4f88ac29b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 9\n",
      "First Chunk:\n",
      "{'text': 'Beyond Earth12Chapter Nubra is a beautiful region in Ladakh. An eleven-year old  girl Yangdol and her twin brother Dorjay live in one of the villages of this region. They love their  surroundingsâ€”the majestic mountain peaks, and the glaciers, but their favourite is the night sky when the entire sky is lit up with thousands of stars (Fig. 12.1). The weather in Nubra is almost cloudless. With almost no air or light pollution, the night sky is very clearly visible. Night after night, Yangdol and Dorjay observe the stars and experience an immense sense of awe. Fig. 12.1: The beauty of night sky from a very dark  location in Ladakh, India Nubra in Ladakh, India Chapter 12.indd   231 10-07-2024   18:13:26  Curiosity | Textbook of Science | Grade 6 232Growing up, Yangdol and Dorjay have been hearing  interesting stories about stars from their elders. They have  heard how some particular stars in the clear skies helped the caravans passing through Nubra in finding direction in the ancient days. They wonder how far away and how big the stars are. They also enjoy trying to find some patterns among the stars that remind them of familiar objects. Have you ever looked at the stars in the night sky and tried to connect them with imaginary lines, just like dots and lines in a drawing? Activity 12.1: Let us draw  \\x8bFig. 12.2 shows bright stars in one part of the night sky. \\x8bLook at it carefully and try to imagine a pattern formed by a group of stars. \\x8bDraw lines to connect the stars and make the pattern. \\x8bThink of an animal or an object that is similar to the pattern drawn by you. Write its name near your pattern.12.1 Stars and Constellations At night, when we look up at the sky, we see many stars. Some stars are bright and others  are dim. Stars shine with their own light. Some groups of stars appear to form  patterns which are like shapes of familiar things. Long ago, when watching stars in the night sky was a favourite pastime of our ancestors, they identified these star patterns with animals, things or characters in stories. Many cultures had names for patterns based on their own stories. These imaginary shapes helped them in recognising stars in the sky. Recognising stars and their patterns was a useful skill  for navigation in the olden times. Before the arrival of modern technology or even before the invention of the magnetic compass, it helped people, particularly sailors and travellers, in finding directions at sea or on land. It is still used in emergencies as a backup method. In earlier times, groups of stars forming patterns were  called constellations. Currently, the regions of sky, which include these groups of stars, are defined as constellations. However , since in constellations, the patterns of stars are often the most prominent, the term constellation is still  commonly used for these groups of stars. Fig. 12.2: A part of the night sky  \\x8bRepeat the above steps and make some more patterns. \\x8bNow think of an interesting story about your patterns. Compare your  patterns with the patterns drawn by your friends. Are the patterns same or different? Narrate your story to others and listen to their stories. Do you notice that everyoneâ€™s patterns, names and stories are different? Is it not fun? Chapter 12.indd   232 10-07-2024   18:13:27 Beyond Earth233Growing up, Yangdol and Dorjay have been hearing  interesting stories about stars from their elders. They have  heard how some particular stars in the clear skies helped the caravans passing through Nubra in finding direction in the ancient days. They wonder how far away and how big the stars are. They also enjoy trying to find some patterns among the stars that remind them of familiar objects. Have you ever looked at the stars in the night sky and tried to connect them with imaginary lines, just like dots and lines in a drawing? Activity 12.1: Let us draw  \\x8bFig. 12.2 shows bright stars in one part of the night sky. \\x8bLook at it carefully and try to imagine a pattern formed by a group of stars. \\x8bDraw lines to connect the stars and make the pattern. \\x8bThink of an animal or an object that is similar to the pattern drawn by you. Write its name near your pattern.12.1 Stars and Constellations At night, when we look up at the sky, we see  many stars. Some stars are bright and others  are dim. Stars shine with their own light.', 'tokens': 988, 'chunk_id': 0, 'page_number': 1}\n"
     ]
    }
   ],
   "source": [
    "from scripts.chunking import dynamic_chunking_with_metadata\n",
    "chunks = dynamic_chunking_with_metadata(text, token_limit=1000, model=\"cl100k_base\",page_number=1)\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(\"First Chunk:\")\n",
    "print(chunks[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f75cf76-73f9-431d-828a-b29381c9ba5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk ID</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>988</td>\n",
       "      <td>Beyond Earth12Chapter Nubra is a beautiful reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>963</td>\n",
       "      <td>Some groups of stars appear to form  patterns ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>970</td>\n",
       "      <td>12.4: Big Dipper , Little Dipper  and Pole Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>996</td>\n",
       "      <td>Activity 12.3: Let us try to identify   Â‹In In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>More to  know! There are many more objects  in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>958</td>\n",
       "      <td>Many Higher Education  Institutions conduct ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>996</td>\n",
       "      <td>Few other comets get broken up, or fall into t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>978</td>\n",
       "      <td>(ii)  Make two similar riddles b y yourself. 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>359</td>\n",
       "      <td>Chapter 12.indd   252 10-07-2024   18:20:23 It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chunk ID  Tokens                                            Preview\n",
       "0         0     988  Beyond Earth12Chapter Nubra is a beautiful reg...\n",
       "1         1     963  Some groups of stars appear to form  patterns ...\n",
       "2         2     970  12.4: Big Dipper , Little Dipper  and Pole Sta...\n",
       "3         3     996  Activity 12.3: Let us try to identify   Â‹In In...\n",
       "4         4    1000  More to  know! There are many more objects  in...\n",
       "5         5     958  Many Higher Education  Institutions conduct ni...\n",
       "6         6     996  Few other comets get broken up, or fall into t...\n",
       "7         7     978  (ii)  Make two similar riddles b y yourself. 3...\n",
       "8         8     359  Chapter 12.indd   252 10-07-2024   18:20:23 It..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Total Chunks: 9\n",
      "ðŸ”  Total Tokens: 8208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize_chunks(chunks):\n",
    "    df = pd.DataFrame([{\n",
    "        \"Chunk ID\": c[\"chunk_id\"],\n",
    "        \"Tokens\": c[\"tokens\"],\n",
    "        \"Preview\": c[\"text\"][:100] + \"...\" if len(c[\"text\"]) > 100 else c[\"text\"]\n",
    "    } for c in chunks])\n",
    "\n",
    "    display(df)\n",
    "    print(f\"ðŸ“¦ Total Chunks: {len(chunks)}\")\n",
    "    print(f\"ðŸ”  Total Tokens: {sum(c['tokens'] for c in chunks)}\")\n",
    "\n",
    "visualize_chunks(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01471483-854a-4f59-a82b-21fbd2063fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23279b09-fbe4-4fe9-8d07-e00aa9890efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b29d835a-2b27-480c-bd91-b04e9d3139b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36f9fe7a-d5eb-4969-af1e-0f837921b2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-chroma -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de5e6870-e994-42ec-8007-fea82f77dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3aaad616-39a2-4876-b033-b4e1fec73ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "âš ï¸ ChromaDB exists at 'vector_db'. Do you want to delete it? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‘ï¸ ChromaDB at 'vector_db' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    confirm = input(f\"âš ï¸ ChromaDB exists at '{db_name}'. Do you want to delete it? (y/n): \")\n",
    "    if confirm.lower() == \"y\":\n",
    "        Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "        print(f\"ðŸ—‘ï¸ ChromaDB at '{db_name}' has been deleted.\")\n",
    "    else:\n",
    "        print(\"âŒ Skipped deletion. Existing DB will be reused.\")\n",
    "else:\n",
    "    print(f\"âœ… No existing ChromaDB found at '{db_name}'. Starting fresh.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dad69b3-9922-4fa8-9331-ff4854d00188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3b9089c-76c1-4aa8-8acf-92e70f1a435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 9 LangChain Documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=chunk[\"text\"],\n",
    "        metadata={\n",
    "            \"chunk_id\": chunk[\"chunk_id\"],\n",
    "            \"page_number\": chunk[\"page_number\"],\n",
    "            \"tokens\": chunk[\"tokens\"]  # Optional: track token count\n",
    "        }\n",
    "    )\n",
    "    for chunk in chunks\n",
    "]\n",
    "\n",
    "print(f\"âœ… Created {len(documents)} LangChain Documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5679f1d6-5a31-4faf-a9dd-3327afc825b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Documents added to ChromaDB successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"./vector_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./vector_db\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"âœ… Documents added to ChromaDB successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe13920a-b36f-4b63-b982-a3c650b250e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-openai chromadb openai tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b56bac8-7334-4816-aa17-24220b02ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install rank_bm25 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c41a1a9-1eed-4c8a-929f-e5e272d41ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 3\n",
    "vector_retriever = vectorstore.as_retriever()\n",
    "vector_retriever.search_kwargs['k'] = 3\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.5, 0.5]  # You can tune this ratio\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c724c82-b4d7-458c-827d-27943b9fc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbe45d16-e527-4a03-af39-14180813195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "759f8079-3d0c-4b2c-93ef-09b0f2f1846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4o-mini\")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1833e9ce-73a4-4a89-95bf-07a2a2e3296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = \"is pdf contains image representation of solar system\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3929e23-57d6-443e-9406-ba1318ac7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"textbook\": \"Generate the answer strictly using the most relevant retrieved chunks from the textbook PDF. Do not include any additional explanation or information not found in the retrieved content, Keep the answer simple, accurate\",\n",
    "    \n",
    "    \"detailed\": \"Using the retrieved textbook content as the main source, explain this in a clearer and slightly more detailed way. Use simple analogies or real-world examples if helpful, but stay close to the textbook content. Assume the reader is a middle school student.\",\n",
    "    \n",
    "    \"advanced\": \"Now go beyond the textbook. Provide a deeper, structured explanation using your knowledge. Include scientific reasoning, historical context, and real-world applications where relevant. Assume the reader is curious and intelligent, like a high school student or above.\"\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt(question, level):\n",
    "    # Clean the input\n",
    "    question = str(question).strip()\n",
    "    prompt_prefix = PROMPTS.get(level, PROMPTS[\"textbook\"])  # Default to textbook if invalid level\n",
    "    \n",
    "    # Build the complete prompt\n",
    "    full_prompt = f\"{prompt_prefix} {question}\"\n",
    "    \n",
    "    return full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07c41f0b-0f30-48b6-9464-10c3ce8ee3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“© User question: Can you explain why the Pole Star is useful?\n",
      "ðŸŽ¯ Level: textbook\n",
      "ðŸ“˜ Using textbook retriever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irfan\\AppData\\Local\\Temp\\ipykernel_25012\\2064501191.py:148: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“© User question: Can you explain why the Pole Star is useful?\n",
      "ðŸŽ¯ Level: detailed\n",
      "ðŸ“— Simplifying PDF-based answer using LLM\n",
      "Alright, so imagine you're lost in the woods and you need to figure out which way is north. The Pole Star, or Polaris, is like a big arrow in the sky that always points north. So if you can find the Pole Star, you can figure out which way is north and then find your way home. People have been using the Pole Star for a really long time to help them know which way to go when they're traveling. It's like having a compass in the sky!ðŸ“© User question: Can you explain why the Pole Star is useful?\n",
      "ðŸŽ¯ Level: advanced\n",
      "ðŸ“• Direct LLM (no PDF) with depth\n",
      "The Pole Star, also known as Polaris, is a star located in the constellation Ursa Minor. It is positioned almost directly above the North Pole, making it a useful navigational tool for travelers in the northern hemisphere. \n",
      "\n",
      "One of the main reasons why the Pole Star is so important is that it remains relatively fixed in the sky while other stars and celestial objects appear to move throughout the night. This means that sailors, hikers, and other travelers can use the Pole Star as a reference point to determine their direction. By locating the Pole Star, they can easily orient themselves and navigate in a specific direction.\n",
      "\n",
      "Fun fact: The Pole Star has been used for navigation by various cultures for centuries. In ancient times, sailors would use the Pole Star along with other stars to navigate the seas. The Polynesian people, for example, used a star compass that included the Pole Star to guide their long-distance voyages across the Pacific Ocean.\n",
      "\n",
      "Overall, the Pole Star's fixed position in the sky and its proximity to the North Pole make it a valuable and reliable tool for navigation in the northern hemisphere.ðŸ“© User question: Are there any other stars or constellations that can help someone determine direction?\n",
      "ðŸŽ¯ Level: advanced\n",
      "ðŸ“• Direct LLM (no PDF) with depth\n",
      "Yes, there are several stars and constellations that have been used for navigation throughout history. One well-known example is the North Star, also known as Polaris, which is located almost directly above the North Pole. Sailors and travelers have used the North Star to determine direction for centuries because it remains relatively fixed in the sky while other stars appear to move throughout the night.\n",
      "\n",
      "Another commonly used constellation for navigation is the Big Dipper, which is part of the larger Ursa Major constellation. The two outer stars in the Dipper's bowl point towards the North Star, making it a helpful tool for finding north.\n",
      "\n",
      "In the Southern Hemisphere, the Southern Cross constellation is often used for navigation. The two brightest stars in the Southern Cross can be used to find the South Celestial Pole, which is the point in the sky directly above the South Pole.\n",
      "\n",
      "Fun fact: The ancient Polynesians were skilled navigators who used the stars, waves, and other natural cues to navigate across the vast Pacific Ocean. They were able to accurately navigate between islands using only their knowledge of the stars and the natural world."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# ðŸ‘‡ Replace this with your actual prompt template\n",
    "SUGGEST_QUESTIONS_PROMPT = \"\"\"\n",
    "Based on the following response, suggest 3 interesting follow-up questions a student might ask to understand more:\n",
    "\n",
    "Response:\n",
    "\"{content}\"\n",
    "\n",
    "Respond with each question on a new line.\n",
    "\"\"\"\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Chat Stream Logic\n",
    "def chat_stream(message, history, level, last_question):\n",
    "    # Use last valid question if message is blank\n",
    "    if not message or len(message.strip()) < 4:\n",
    "        if not last_question or len(last_question.strip()) < 4:\n",
    "            yield history, history, last_question  # No valid question to run\n",
    "            return\n",
    "        message = last_question\n",
    "    else:\n",
    "        last_question = message\n",
    "\n",
    "    print(f\"ðŸ“© User question: {message}\")\n",
    "    print(f\"ðŸŽ¯ Level: {level}\")\n",
    "\n",
    "    try:\n",
    "        if level == \"textbook\":\n",
    "            print(\"ðŸ“˜ Using textbook retriever\")\n",
    "            result = conversation_chain.invoke({\n",
    "                \"question\": message,\n",
    "                \"chat_history\": history\n",
    "            })\n",
    "            answer = result[\"answer\"]\n",
    "\n",
    "            new_messages = [\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ]\n",
    "            yield history + new_messages, history + new_messages, last_question\n",
    "\n",
    "        elif level == \"detailed\":\n",
    "            print(\"ðŸ“— Simplifying PDF-based answer using LLM\")\n",
    "            base_result = conversation_chain.invoke({\n",
    "                \"question\": message,\n",
    "                \"chat_history\": history\n",
    "            })\n",
    "            base_answer = base_result[\"answer\"]\n",
    "\n",
    "            history += [\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "                {\"role\": \"assistant\", \"content\": base_answer}\n",
    "            ]\n",
    "\n",
    "            llm = ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                streaming=True,\n",
    "                temperature=0.5,\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                callbacks=[StreamingStdOutCallbackHandler()]\n",
    "            )\n",
    "\n",
    "            final_answer = \"\"\n",
    "            for chunk in llm.stream([HumanMessage(content=f\"Explain this like you're teaching a 6th grader:\\n{base_answer}\")]):\n",
    "                final_answer += chunk.content\n",
    "                yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], last_question\n",
    "\n",
    "        elif level == \"advanced\":\n",
    "            print(\"ðŸ“• Direct LLM (no PDF) with depth\")\n",
    "            llm = ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                streaming=True,\n",
    "                temperature=0.7,\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                callbacks=[StreamingStdOutCallbackHandler()]\n",
    "            )\n",
    "\n",
    "            final_answer = \"\"\n",
    "            for chunk in llm.stream([HumanMessage(content=f\"Explain this with real-world insights and fun facts:\\n{message}\")]):\n",
    "                final_answer += chunk.content\n",
    "                yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], last_question\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR: {e}\")\n",
    "        error_msg = f\"âš ï¸ Something went wrong: {str(e)}\"\n",
    "        yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": error_msg}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": error_msg}], last_question\n",
    "\n",
    "\n",
    "\n",
    "# Save & Load Chat Functions\n",
    "def save_chat_to_file(history):\n",
    "    output_dir = \"D:/BuddyChats\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"Buddy_Chat_{timestamp}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"ðŸ“ Saved file to:\", filepath)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_chat_from_file(file_obj):\n",
    "    if not file_obj or not hasattr(file_obj, \"name\"):\n",
    "        print(\"âš ï¸ No file uploaded\")\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "        with open(file_obj.name, \"r\", encoding=\"utf-8\") as f:\n",
    "            history = json.load(f)\n",
    "\n",
    "        if isinstance(history, list):\n",
    "            if all(isinstance(pair, list) and len(pair) == 2 for pair in history):\n",
    "                message_format = []\n",
    "                for pair in history:\n",
    "                    message_format.append({\"role\": \"user\", \"content\": pair[0]})\n",
    "                    message_format.append({\"role\": \"assistant\", \"content\": pair[1]})\n",
    "                return message_format, message_format\n",
    "            elif all(isinstance(m, dict) and \"role\" in m and \"content\" in m for m in history):\n",
    "                return history, history\n",
    "\n",
    "        print(\"âš ï¸ Unsupported format in loaded chat\")\n",
    "        return [], []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading chat file: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def generate_suggested_questions(content):\n",
    "    try:\n",
    "        prompt = SUGGEST_QUESTIONS_PROMPT.format(content=str(content).strip())\n",
    "        llm = ChatOpenAI(\n",
    "            temperature=0.7,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        response = llm.invoke(prompt)\n",
    "        suggestions = response.content.strip().split(\"\\n\")\n",
    "        cleaned = [s.strip(\" -â€¢123.\").strip() for s in suggestions if s.strip() and len(s.strip()) > 5]\n",
    "        while len(cleaned) < 3:\n",
    "            cleaned.append(\"\")\n",
    "        return cleaned[:3]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating suggestions: {str(e)}\")\n",
    "        return [\"\", \"\", \"\"]\n",
    "        \n",
    "# UI\n",
    "with gr.Blocks(css=... ) as demo:  # Keep your existing CSS here\n",
    "\n",
    "    gr.Markdown(\"### ðŸ‘‹ Welcome to Buddy AI: Your Learning Companion\")\n",
    "\n",
    "    question = gr.Textbox(label=\"Ask your question ðŸ‘‡\", lines=1)\n",
    "    level = gr.Dropdown(choices=[\"textbook\", \"detailed\", \"advanced\"], value=\"textbook\", label=\"Answer Level ðŸŽ¯\")\n",
    "    chat_ui = gr.Chatbot(label=\"ðŸ§  Buddy AI Conversation\", type=\"messages\")\n",
    "    state = gr.State([])\n",
    "    last_question = gr.State(\"\") \n",
    "\n",
    "    suggested_q1 = gr.State()\n",
    "    suggested_q2 = gr.State()\n",
    "    suggested_q3 = gr.State()\n",
    "\n",
    "    suggest_heading = gr.Markdown(\"#### ðŸ¤” Suggested Follow-up Questions\", visible=False)\n",
    "    suggest_btn1 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "    suggest_btn2 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "    suggest_btn3 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            submit_btn = gr.Button(\"âœ… Submit\", elem_classes=\"gr-button-primary\")\n",
    "            clear_btn = gr.Button(\"âŒ Clear\")\n",
    "        with gr.Column(scale=1):\n",
    "            save_btn = gr.Button(\"ðŸ’¾ Save Chat\")\n",
    "            save_status = gr.Markdown(visible=False)\n",
    "            load_file = gr.File(label=\"ðŸ“‚ Load Previous Chat (.json)\", file_types=[\".json\"])\n",
    "            load_btn = gr.Button(\"ðŸ“‚ Load Chat\")\n",
    "\n",
    "    chat_event = submit_btn.click(\n",
    "        fn=chat_stream,\n",
    "        inputs=[question, state, level, last_question],\n",
    "        outputs=[chat_ui, state, question],\n",
    "        api_name=\"chat_stream\"\n",
    "    ).then(\n",
    "        fn=lambda q: q,  # update last_question\n",
    "        inputs=[question],\n",
    "        outputs=[last_question]\n",
    ")\n",
    "     # Auto-submit when answer level changes\n",
    "    \n",
    "    def extract_last_bot_reply(chat_history):\n",
    "        if isinstance(chat_history, list):\n",
    "            if len(chat_history) > 0 and isinstance(chat_history[-1], list):\n",
    "                return chat_history[-1][1]\n",
    "            for msg in reversed(chat_history):\n",
    "                if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "                    return msg.get(\"content\", \"\")\n",
    "        return \"\"\n",
    "    def suggest_followups(chat_history):\n",
    "        last_bot_reply = extract_last_bot_reply(chat_history)\n",
    "        suggestions = generate_suggested_questions(last_bot_reply)\n",
    "        s1 = gr.update(value=suggestions[0], visible=True) if suggestions else gr.update(visible=False)\n",
    "        s2 = gr.update(value=suggestions[1], visible=True) if len(suggestions) > 1 else gr.update(visible=False)\n",
    "        s3 = gr.update(value=suggestions[2], visible=True) if len(suggestions) > 2 else gr.update(visible=False)\n",
    "        return s1, s2, s3, suggestions[0], suggestions[1], suggestions[2], gr.update(visible=True)\n",
    "    \n",
    "    chat_event.then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "    level.change(\n",
    "        fn=chat_stream,\n",
    "        inputs=[question, state, level, last_question],\n",
    "        outputs=[chat_ui, state, question]\n",
    "    ).then(\n",
    "        fn=lambda q: q,\n",
    "        inputs=[question],\n",
    "        outputs=[last_question]\n",
    "    ).then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "    suggest_btn1.click(chat_stream, inputs=[suggested_q1, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "    suggest_btn2.click(chat_stream, inputs=[suggested_q2, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "    suggest_btn3.click(chat_stream, inputs=[suggested_q3, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda: ([], [], \"\", \"\", \"\", \"\", \"\", \"\", \"\", gr.update(visible=False)),\n",
    "        inputs=[],\n",
    "        outputs=[chat_ui, state, question, suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "    save_btn.click(\n",
    "        fn=save_chat_to_file,\n",
    "        inputs=[state],\n",
    "        outputs=[save_status]\n",
    "    ).then(\n",
    "        fn=lambda f: gr.update(visible=True, value=f\"âœ… Chat saved to: `{f}`\"),\n",
    "        inputs=[save_status],\n",
    "        outputs=[save_status]\n",
    "    )\n",
    "\n",
    "    load_btn.click(\n",
    "        fn=load_chat_from_file,\n",
    "        inputs=[load_file],\n",
    "        outputs=[chat_ui, state]\n",
    "    ).then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "demo.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127b69a-48b3-48c1-a063-4b9eb804dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f52af3-3a33-4db6-a5a6-2b4fce41e269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db28d83f-1290-4712-9a80-5e272f928152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python: C:\\Users\\Irfan\\python.exe\n",
      "Virtual Env: C:\\Users\\Irfan\n",
      "Working Dir: D:\\work\\LONGSPUR\\TASK\\AI_buddy_model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(f\"\"\"\n",
    "Python: {sys.executable}\n",
    "Virtual Env: {sys.prefix}\n",
    "Working Dir: {os.getcwd()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682e529a-275e-4a1f-b09d-1075af280fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irfan\\Lib\\site-packages\\pandas\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__file__)  # Should show buddyvenv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1f9ff1-c708-4b7b-ac76-a519fdba4b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2163967-2d95-4273-aa0e-261966d0bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --force-reinstall python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71046d2c-ef8a-402b-990c-f5dcfe4aafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06a0f2a9-7f45-4aa3-9193-a9c2a0c4093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')  \n",
    "if openai_api_key:\n",
    "    print(\"API key loaded successfully!\")\n",
    "else:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d16ba81-0c8c-4899-902a-da510f80751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-gKZ3JoqWn7Mvkbrwako2Dh3JQ5_Q3ITj_kg-UihJZyT3BlbkFJpH3lElvBx274mbO5N7oIk-9GLZddYI3ZtWbWmg3tEA\n",
      "üì© User question: what is solar system?\n",
      "üéØ Level: textbook\n",
      "üìò Using textbook retriever\n",
      "‚ùå ERROR: Completions.create() got an unexpected keyword argument 'system'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irfan\\AppData\\Local\\Temp\\ipykernel_8040\\2064501191.py:148: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì© User question: hi there?\n",
      "üéØ Level: textbook\n",
      "üìò Using textbook retriever\n",
      "‚ùå ERROR: Completions.create() got an unexpected keyword argument 'system'\n"
     ]
    }
   ],
   "source": [
    "print(openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf98451f-4a29-4b55-b697-755fd890ba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.75.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: Apache-2.0\n",
      "Location: C:\\Users\\Irfan\\Lib\\site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc71bb9-4766-4c64-9c2f-702197087420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "#from langchain_core.documents import Document\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c797f683-cec8-4e79-978f-ccddf3e9c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ca32697-d82a-42a4-a9f2-e80716808460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f18f8d74-85b2-41f6-be51-1dd20c773a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess_pdf import extract_text_from_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf3824c-e46e-4a69-a2b9-d588af160902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyond Earth12Chapter\n",
      "Nubra is a beautiful region in Ladakh. An eleven-year old \n",
      "girl Yangdol and he\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_path = os.path.join(\"knowledge-base\", \"textbook.pdf\")\n",
    "pdf_path = os.path.abspath(base_path) \n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "if text:\n",
    "    print(text[:100])\n",
    "else:\n",
    "    print(\"No text extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec6c42a7-f550-4477-9875-47320c63f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ADVANCED = False  \n",
    "MODEL = \"gpt-4o-mini\" if USE_ADVANCED else \"gpt-3.5-turbo\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818bb03f-f400-437f-8c8a-3179185ea9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3c4f09-0dfb-4110-ac49-c879bc83337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:\\LONGSPUR\\TASK\\AI_buddy_model\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk_data_dir = \"D:\\\\LONGSPUR\\\\TASK\\\\AI_buddy_model\\\\nltk_data\"\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.data.path.append(nltk_data_dir)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b69a33c8-8de6-48fd-889a-7414fb218eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import encoding_for_model\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0013c404-24bd-4200-9598-e4f88ac29b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total Chunks Created: 9\n",
      "Chunk 0: Beyond Earth12Chapter Nubra is a beautiful region in Ladakh. An eleven-year old  girl Yangdol and he...\n",
      "Chunk 1: Some groups of stars appear to form  patterns which are like shapes of familiar things. Long ago, wh...\n",
      "Chunk 2: 12.4: Big Dipper , Little Dipper  and Pole Star (The lines are not seen  in the sky and have been dr...\n",
      "Chunk 3: Activity 12.3: Let us try to identify   ¬ãIn India, Orion is best viewed during the months of Decembe...\n",
      "Chunk 4: More to  know! There are many more objects  in the sky. Our Earth, along  with some of these objects...\n",
      "Total Chunks: 9\n",
      "First Chunk:\n",
      "{'chunk_id': '1-0', 'text': 'Beyond Earth12Chapter Nubra is a beautiful region in Ladakh. An eleven-year old  girl Yangdol and her twin brother Dorjay live in one of the villages of this region. They love their  surroundings‚Äîthe majestic mountain peaks, and the glaciers, but their favourite is the night sky when the entire sky is lit up with thousands of stars (Fig. 12.1). The weather in Nubra is almost cloudless. With almost no air or light pollution, the night sky is very clearly visible. Night after night, Yangdol and Dorjay observe the stars and experience an immense sense of awe. Fig. 12.1: The beauty of night sky from a very dark  location in Ladakh, India Nubra in Ladakh, India Chapter 12.indd   231 10-07-2024   18:13:26  Curiosity | Textbook of Science | Grade 6 232Growing up, Yangdol and Dorjay have been hearing  interesting stories about stars from their elders. They have  heard how some particular stars in the clear skies helped the caravans passing through Nubra in finding direction in the ancient days. They wonder how far away and how big the stars are. They also enjoy trying to find some patterns among the stars that remind them of familiar objects. Have you ever looked at the stars in the night sky and tried to connect them with imaginary lines, just like dots and lines in a drawing? Activity 12.1: Let us draw  \\x8bFig. 12.2 shows bright stars in one part of the night sky. \\x8bLook at it carefully and try to imagine a pattern formed by a group of stars. \\x8bDraw lines to connect the stars and make the pattern. \\x8bThink of an animal or an object that is similar to the pattern drawn by you. Write its name near your pattern.12.1 Stars and Constellations At night, when we look up at the sky, we see many stars. Some stars are bright and others  are dim. Stars shine with their own light. Some groups of stars appear to form  patterns which are like shapes of familiar things. Long ago, when watching stars in the night sky was a favourite pastime of our ancestors, they identified these star patterns with animals, things or characters in stories. Many cultures had names for patterns based on their own stories. These imaginary shapes helped them in recognising stars in the sky. Recognising stars and their patterns was a useful skill  for navigation in the olden times. Before the arrival of modern technology or even before the invention of the magnetic compass, it helped people, particularly sailors and travellers, in finding directions at sea or on land. It is still used in emergencies as a backup method. In earlier times, groups of stars forming patterns were  called constellations. Currently, the regions of sky, which include these groups of stars, are defined as constellations. However , since in constellations, the patterns of stars are often the most prominent, the term constellation is still  commonly used for these groups of stars. Fig. 12.2: A part of the night sky  \\x8bRepeat the above steps and make some more patterns. \\x8bNow think of an interesting story about your patterns. Compare your  patterns with the patterns drawn by your friends. Are the patterns same or different? Narrate your story to others and listen to their stories. Do you notice that everyone‚Äôs patterns, names and stories are different? Is it not fun? Chapter 12.indd   232 10-07-2024   18:13:27 Beyond Earth233Growing up, Yangdol and Dorjay have been hearing  interesting stories about stars from their elders. They have  heard how some particular stars in the clear skies helped the caravans passing through Nubra in finding direction in the ancient days. They wonder how far away and how big the stars are. They also enjoy trying to find some patterns among the stars that remind them of familiar objects. Have you ever looked at the stars in the night sky and tried to connect them with imaginary lines, just like dots and lines in a drawing? Activity 12.1: Let us draw  \\x8bFig. 12.2 shows bright stars in one part of the night sky. \\x8bLook at it carefully and try to imagine a pattern formed by a group of stars. \\x8bDraw lines to connect the stars and make the pattern. \\x8bThink of an animal or an object that is similar to the pattern drawn by you. Write its name near your pattern.12.1 Stars and Constellations At night, when we look up at the sky, we see  many stars. Some stars are bright and others  are dim. Stars shine with their own light.', 'tokens': 988, 'page_number': 1}\n"
     ]
    }
   ],
   "source": [
    "from scripts.chunking import dynamic_chunking_with_metadata\n",
    "chunks = dynamic_chunking_with_metadata(text, token_limit=1000, model=\"cl100k_base\",page_number=1)\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(\"First Chunk:\")\n",
    "print(chunks[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f75cf76-73f9-431d-828a-b29381c9ba5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk ID</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-0</td>\n",
       "      <td>988</td>\n",
       "      <td>Beyond Earth12Chapter Nubra is a beautiful reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-1</td>\n",
       "      <td>963</td>\n",
       "      <td>Some groups of stars appear to form  patterns ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-2</td>\n",
       "      <td>970</td>\n",
       "      <td>12.4: Big Dipper , Little Dipper  and Pole Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-3</td>\n",
       "      <td>996</td>\n",
       "      <td>Activity 12.3: Let us try to identify   ¬ãIn In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-4</td>\n",
       "      <td>1000</td>\n",
       "      <td>More to  know! There are many more objects  in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1-5</td>\n",
       "      <td>958</td>\n",
       "      <td>Many Higher Education  Institutions conduct ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1-6</td>\n",
       "      <td>996</td>\n",
       "      <td>Few other comets get broken up, or fall into t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1-7</td>\n",
       "      <td>978</td>\n",
       "      <td>(ii)  Make two similar riddles b y yourself. 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1-8</td>\n",
       "      <td>359</td>\n",
       "      <td>Chapter 12.indd   252 10-07-2024   18:20:23 It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Chunk ID  Tokens                                            Preview\n",
       "0      1-0     988  Beyond Earth12Chapter Nubra is a beautiful reg...\n",
       "1      1-1     963  Some groups of stars appear to form  patterns ...\n",
       "2      1-2     970  12.4: Big Dipper , Little Dipper  and Pole Sta...\n",
       "3      1-3     996  Activity 12.3: Let us try to identify   ¬ãIn In...\n",
       "4      1-4    1000  More to  know! There are many more objects  in...\n",
       "5      1-5     958  Many Higher Education  Institutions conduct ni...\n",
       "6      1-6     996  Few other comets get broken up, or fall into t...\n",
       "7      1-7     978  (ii)  Make two similar riddles b y yourself. 3...\n",
       "8      1-8     359  Chapter 12.indd   252 10-07-2024   18:20:23 It..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Total Chunks: 9\n",
      "üî† Total Tokens: 8208\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize_chunks(chunks):\n",
    "    df = pd.DataFrame([{\n",
    "        \"Chunk ID\": c[\"chunk_id\"],\n",
    "        \"Tokens\": c[\"tokens\"],\n",
    "        \"Preview\": c[\"text\"][:100] + \"...\" if len(c[\"text\"]) > 100 else c[\"text\"]\n",
    "    } for c in chunks])\n",
    "\n",
    "    display(df)\n",
    "    print(f\"üì¶ Total Chunks: {len(chunks)}\")\n",
    "    print(f\"üî† Total Tokens: {sum(c['tokens'] for c in chunks)}\")\n",
    "\n",
    "visualize_chunks(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01471483-854a-4f59-a82b-21fbd2063fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-chroma 0.2.2 requires chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0, but you have chromadb 1.0.5 which is incompatible.\n",
      "whisperflow 1.0.0 requires fastapi==0.108.0, but you have fastapi 0.115.9 which is incompatible.\n",
      "whisperflow 1.0.0 requires python-multipart==0.0.9, but you have python-multipart 0.0.20 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23279b09-fbe4-4fe9-8d07-e00aa9890efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b29d835a-2b27-480c-bd91-b04e9d3139b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f9fe7a-d5eb-4969-af1e-0f837921b2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-chroma -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de5e6870-e994-42ec-8007-fea82f77dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3aaad616-39a2-4876-b033-b4e1fec73ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è ChromaDB exists at 'vector_db'. Do you want to delete it? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è ChromaDB at 'vector_db' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    confirm = input(f\"‚ö†Ô∏è ChromaDB exists at '{db_name}'. Do you want to delete it? (y/n): \")\n",
    "    if confirm.lower() == \"y\":\n",
    "        Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "        print(f\"üóëÔ∏è ChromaDB at '{db_name}' has been deleted.\")\n",
    "    else:\n",
    "        print(\"‚ùå Skipped deletion. Existing DB will be reused.\")\n",
    "else:\n",
    "    print(f\"‚úÖ No existing ChromaDB found at '{db_name}'. Starting fresh.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dad69b3-9922-4fa8-9331-ff4854d00188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3b9089c-76c1-4aa8-8acf-92e70f1a435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 9 LangChain Documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=chunk[\"text\"],\n",
    "        metadata={\n",
    "            \"chunk_id\": chunk[\"chunk_id\"],\n",
    "            \"page_number\": chunk[\"page_number\"],\n",
    "            \"tokens\": chunk[\"tokens\"]  # Optional: track token count\n",
    "        }\n",
    "    )\n",
    "    for chunk in chunks\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(documents)} LangChain Documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5679f1d6-5a31-4faf-a9dd-3327afc825b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Documents added to ChromaDB successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = \"./vector_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./vector_db\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Documents added to ChromaDB successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe13920a-b36f-4b63-b982-a3c650b250e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-openai chromadb openai tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b56bac8-7334-4816-aa17-24220b02ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rank_bm25 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c41a1a9-1eed-4c8a-929f-e5e272d41ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 3\n",
    "vector_retriever = vectorstore.as_retriever()\n",
    "vector_retriever.search_kwargs['k'] = 3\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.5, 0.5]  # You can tune this ratio\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c724c82-b4d7-458c-827d-27943b9fc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbe45d16-e527-4a03-af39-14180813195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "759f8079-3d0c-4b2c-93ef-09b0f2f1846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irfan\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3489: UserWarning: WARNING! system is not default parameter.\n",
      "                system was transferred to model_kwargs.\n",
      "                Please confirm that system is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "C:\\Users\\Irfan\\AppData\\Local\\Temp\\ipykernel_8040\\3240765173.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    system=\"You are a helpful tutor. Always respond in English.\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3929e23-57d6-443e-9406-ba1318ac7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    \"textbook\": \"Generate the answer strictly using the most relevant retrieved chunks from the textbook PDF. Do not include any additional explanation or information not found in the retrieved content, Keep the answer simple, accurate\",\n",
    "    \n",
    "    \"detailed\": \"Using the retrieved textbook content as the main source, explain this in a clearer and slightly more detailed way. Use simple analogies or real-world examples if helpful, but stay close to the textbook content. Assume the reader is a middle school student.\",\n",
    "    \n",
    "    \"advanced\": \"Now go beyond the textbook. Provide a deeper, structured explanation using your knowledge. Include scientific reasoning, historical context, and real-world applications where relevant. Assume the reader is curious and intelligent, like a high school student or above.\"\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt(question, level):\n",
    "    # Clean the input\n",
    "    question = str(question).strip()\n",
    "    prompt_prefix = PROMPTS.get(level, PROMPTS[\"textbook\"])  # Default to textbook if invalid level\n",
    "    \n",
    "    # Build the complete prompt\n",
    "    full_prompt = f\"{prompt_prefix} {question}\"\n",
    "    \n",
    "    return full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07c41f0b-0f30-48b6-9464-10c3ce8ee3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# üëá Replace this with your actual prompt template\n",
    "SUGGEST_QUESTIONS_PROMPT = \"\"\"\n",
    "Based on the following response, suggest 3 interesting follow-up questions a student might ask to understand more:\n",
    "\n",
    "Response:\n",
    "\"{content}\"\n",
    "\n",
    "Respond with each question on a new line.\n",
    "\"\"\"\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Chat Stream Logic\n",
    "def chat_stream(message, history, level, last_question):\n",
    "    # Use last valid question if message is blank\n",
    "    if not message or len(message.strip()) < 4:\n",
    "        if not last_question or len(last_question.strip()) < 4:\n",
    "            yield history, history, last_question  # No valid question to run\n",
    "            return\n",
    "        message = last_question\n",
    "    else:\n",
    "        last_question = message\n",
    "\n",
    "    print(f\"üì© User question: {message}\")\n",
    "    print(f\"üéØ Level: {level}\")\n",
    "\n",
    "    try:\n",
    "        if level == \"textbook\":\n",
    "            print(\"üìò Using textbook retriever\")\n",
    "            result = conversation_chain.invoke({\n",
    "                \"question\": message,\n",
    "                \"chat_history\": history\n",
    "            })\n",
    "            answer = result[\"answer\"]\n",
    "\n",
    "            new_messages = [\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ]\n",
    "            yield history + new_messages, history + new_messages, last_question\n",
    "\n",
    "        elif level == \"detailed\":\n",
    "            print(\"üìó Simplifying PDF-based answer using LLM\")\n",
    "            base_result = conversation_chain.invoke({\n",
    "                \"question\": message,\n",
    "                \"chat_history\": history\n",
    "            })\n",
    "            base_answer = base_result[\"answer\"]\n",
    "\n",
    "            history += [\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "                {\"role\": \"assistant\", \"content\": base_answer}\n",
    "            ]\n",
    "\n",
    "            llm = ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                streaming=True,\n",
    "                temperature=0.5,\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                callbacks=[StreamingStdOutCallbackHandler()]\n",
    "            )\n",
    "\n",
    "            final_answer = \"\"\n",
    "            for chunk in llm.stream([HumanMessage(content=f\"Explain this like you're teaching a 6th grader:\\n{base_answer}\")]):\n",
    "                final_answer += chunk.content\n",
    "                yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], last_question\n",
    "\n",
    "        elif level == \"advanced\":\n",
    "            print(\"üìï Direct LLM (no PDF) with depth\")\n",
    "            llm = ChatOpenAI(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                streaming=True,\n",
    "                temperature=0.7,\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                callbacks=[StreamingStdOutCallbackHandler()]\n",
    "            )\n",
    "\n",
    "            final_answer = \"\"\n",
    "            for chunk in llm.stream([HumanMessage(content=f\"Explain this with real-world insights and fun facts:\\n{message}\")]):\n",
    "                final_answer += chunk.content\n",
    "                yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": final_answer}], last_question\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: {e}\")\n",
    "        error_msg = f\"‚ö†Ô∏è Something went wrong: {str(e)}\"\n",
    "        yield history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": error_msg}], history + [{\"role\": \"user\", \"content\": message}, {\"role\": \"assistant\", \"content\": error_msg}], last_question\n",
    "\n",
    "\n",
    "\n",
    "# Save & Load Chat Functions\n",
    "def save_chat_to_file(history):\n",
    "    output_dir = \"D:/BuddyChats\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    filename = f\"Buddy_Chat_{timestamp}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"üìÅ Saved file to:\", filepath)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def load_chat_from_file(file_obj):\n",
    "    if not file_obj or not hasattr(file_obj, \"name\"):\n",
    "        print(\"‚ö†Ô∏è No file uploaded\")\n",
    "        return [], []\n",
    "\n",
    "    try:\n",
    "        with open(file_obj.name, \"r\", encoding=\"utf-8\") as f:\n",
    "            history = json.load(f)\n",
    "\n",
    "        if isinstance(history, list):\n",
    "            if all(isinstance(pair, list) and len(pair) == 2 for pair in history):\n",
    "                message_format = []\n",
    "                for pair in history:\n",
    "                    message_format.append({\"role\": \"user\", \"content\": pair[0]})\n",
    "                    message_format.append({\"role\": \"assistant\", \"content\": pair[1]})\n",
    "                return message_format, message_format\n",
    "            elif all(isinstance(m, dict) and \"role\" in m and \"content\" in m for m in history):\n",
    "                return history, history\n",
    "\n",
    "        print(\"‚ö†Ô∏è Unsupported format in loaded chat\")\n",
    "        return [], []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading chat file: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "def generate_suggested_questions(content):\n",
    "    try:\n",
    "        prompt = SUGGEST_QUESTIONS_PROMPT.format(content=str(content).strip())\n",
    "        llm = ChatOpenAI(\n",
    "            temperature=0.7,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        response = llm.invoke(prompt)\n",
    "        suggestions = response.content.strip().split(\"\\n\")\n",
    "        cleaned = [s.strip(\" -‚Ä¢123.\").strip() for s in suggestions if s.strip() and len(s.strip()) > 5]\n",
    "        while len(cleaned) < 3:\n",
    "            cleaned.append(\"\")\n",
    "        return cleaned[:3]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating suggestions: {str(e)}\")\n",
    "        return [\"\", \"\", \"\"]\n",
    "        \n",
    "# UI\n",
    "with gr.Blocks(css=... ) as demo:  # Keep your existing CSS here\n",
    "\n",
    "    gr.Markdown(\"### üëã Welcome to Buddy AI: Your Learning Companion\")\n",
    "\n",
    "    question = gr.Textbox(label=\"Ask your question üëá\", lines=1)\n",
    "    level = gr.Dropdown(choices=[\"textbook\", \"detailed\", \"advanced\"], value=\"textbook\", label=\"Answer Level üéØ\")\n",
    "    chat_ui = gr.Chatbot(label=\"üß† Buddy AI Conversation\", type=\"messages\")\n",
    "    state = gr.State([])\n",
    "    last_question = gr.State(\"\") \n",
    "\n",
    "    suggested_q1 = gr.State()\n",
    "    suggested_q2 = gr.State()\n",
    "    suggested_q3 = gr.State()\n",
    "\n",
    "    suggest_heading = gr.Markdown(\"#### ü§î Suggested Follow-up Questions\", visible=False)\n",
    "    suggest_btn1 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "    suggest_btn2 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "    suggest_btn3 = gr.Button(visible=False, interactive=True, elem_classes=\"suggested-btn\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            submit_btn = gr.Button(\"‚úÖ Submit\", elem_classes=\"gr-button-primary\")\n",
    "            clear_btn = gr.Button(\"‚ùå Clear\")\n",
    "        with gr.Column(scale=1):\n",
    "            save_btn = gr.Button(\"üíæ Save Chat\")\n",
    "            save_status = gr.Markdown(visible=False)\n",
    "            load_file = gr.File(label=\"üìÇ Load Previous Chat (.json)\", file_types=[\".json\"])\n",
    "            load_btn = gr.Button(\"üìÇ Load Chat\")\n",
    "\n",
    "    chat_event = submit_btn.click(\n",
    "        fn=chat_stream,\n",
    "        inputs=[question, state, level, last_question],\n",
    "        outputs=[chat_ui, state, question],\n",
    "        api_name=\"chat_stream\"\n",
    "    ).then(\n",
    "        fn=lambda q: q,  # update last_question\n",
    "        inputs=[question],\n",
    "        outputs=[last_question]\n",
    ")\n",
    "     # Auto-submit when answer level changes\n",
    "    \n",
    "    def extract_last_bot_reply(chat_history):\n",
    "        if isinstance(chat_history, list):\n",
    "            if len(chat_history) > 0 and isinstance(chat_history[-1], list):\n",
    "                return chat_history[-1][1]\n",
    "            for msg in reversed(chat_history):\n",
    "                if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "                    return msg.get(\"content\", \"\")\n",
    "        return \"\"\n",
    "    def suggest_followups(chat_history):\n",
    "        last_bot_reply = extract_last_bot_reply(chat_history)\n",
    "        suggestions = generate_suggested_questions(last_bot_reply)\n",
    "        s1 = gr.update(value=suggestions[0], visible=True) if suggestions else gr.update(visible=False)\n",
    "        s2 = gr.update(value=suggestions[1], visible=True) if len(suggestions) > 1 else gr.update(visible=False)\n",
    "        s3 = gr.update(value=suggestions[2], visible=True) if len(suggestions) > 2 else gr.update(visible=False)\n",
    "        return s1, s2, s3, suggestions[0], suggestions[1], suggestions[2], gr.update(visible=True)\n",
    "    \n",
    "    chat_event.then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "    level.change(\n",
    "        fn=chat_stream,\n",
    "        inputs=[question, state, level, last_question],\n",
    "        outputs=[chat_ui, state, question]\n",
    "    ).then(\n",
    "        fn=lambda q: q,\n",
    "        inputs=[question],\n",
    "        outputs=[last_question]\n",
    "    ).then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "    suggest_btn1.click(chat_stream, inputs=[suggested_q1, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "    suggest_btn2.click(chat_stream, inputs=[suggested_q2, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "    suggest_btn3.click(chat_stream, inputs=[suggested_q3, state, level, last_question], outputs=[chat_ui, state, question])\n",
    "\n",
    "\n",
    "    clear_btn.click(\n",
    "        fn=lambda: ([], [], \"\", \"\", \"\", \"\", \"\", \"\", \"\", gr.update(visible=False)),\n",
    "        inputs=[],\n",
    "        outputs=[chat_ui, state, question, suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "    save_btn.click(\n",
    "        fn=save_chat_to_file,\n",
    "        inputs=[state],\n",
    "        outputs=[save_status]\n",
    "    ).then(\n",
    "        fn=lambda f: gr.update(visible=True, value=f\"‚úÖ Chat saved to: `{f}`\"),\n",
    "        inputs=[save_status],\n",
    "        outputs=[save_status]\n",
    "    )\n",
    "\n",
    "    load_btn.click(\n",
    "        fn=load_chat_from_file,\n",
    "        inputs=[load_file],\n",
    "        outputs=[chat_ui, state]\n",
    "    ).then(\n",
    "        fn=suggest_followups,\n",
    "        inputs=[chat_ui],\n",
    "        outputs=[suggest_btn1, suggest_btn2, suggest_btn3, suggested_q1, suggested_q2, suggested_q3, suggest_heading]\n",
    "    )\n",
    "\n",
    "demo.launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127b69a-48b3-48c1-a063-4b9eb804dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f52af3-3a33-4db6-a5a6-2b4fce41e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4350e65-16a2-4838-9085-b50e0ff86baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32720cb7-f20d-4ca9-973b-b43d97102607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
